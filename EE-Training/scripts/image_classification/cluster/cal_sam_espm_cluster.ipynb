{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d0de49",
   "metadata": {},
   "source": [
    "# Calculate SAM and ESPM – Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc7587",
   "metadata": {},
   "source": [
    "### Check All exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85892ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet_b3_imagenet-1k_300ep_bs512.csv\n",
      "convnext_base_imagenet-1k_300ep_bs128.csv\n",
      "convnext_base_imagenet-1k_300ep_bs512.csv\n",
      "swin_large_patch4_window7_224_imagenet-1k_300ep_bs256.csv\n",
      "deit_base_patch16_224_imagenet-1k_300ep_bs256.csv\n",
      "convnext_xlarge_imagenet-1k_300ep_bs128.csv\n",
      "vit_large_patch16_224_imagenet-1k_300ep_bs256.csv\n",
      "tiny_vit_21m_224_imagenet-1k_300ep_bs256.csv\n",
      "mae_vit_base_patch16_imagenet-1k_300ep_bs256.csv\n",
      "mae_vit_base_patch16_imagenet-1k_300ep_bs128.csv\n",
      "resnet101_imagenet-1k_300ep_bs512.csv\n",
      "resnet101_imagenet-1k_300ep_bs128.csv\n",
      "efficientnet_v2_s_imagenet-1k_300ep_bs256.csv\n",
      "mae_vit_base_patch16_imagenet-1k_300ep_bs512.csv\n",
      "deit_large_patch16_224_imagenet-1k_300ep_bs256.csv\n",
      "beit_base_patch16_224_imagenet-1k_300ep_bs128.csv\n",
      "beit_base_patch16_224_imagenet-1k_300ep_bs256.csv\n",
      "beit_large_patch16_224_imagenet-1k_300ep_bs256.csv\n",
      "beit_large_patch16_224_imagenet-1k_300ep_bs512.csv\n",
      "vit_huge_patch14_224_imagenet-1k_300ep_bs256.csv\n",
      "efficientnet_b3_imagenet-1k_300ep_bs128.csv\n",
      "convnext_large_imagenet-1k_300ep_bs512.csv\n",
      "convnext_xlarge_imagenet-1k_300ep_bs256.csv\n",
      "deit_large_patch16_224_imagenet-1k_300ep_bs512.csv\n",
      "regnet_y_16gf_imagenet-1k_300ep_bs512.csv\n",
      "convnext_large_imagenet-1k_300ep_bs128.csv\n",
      "convnext_base_imagenet-1k_300ep_bs256.csv\n",
      "vit_base_patch16_224_imagenet-1k_300ep_bs128.csv\n",
      "deit_base_patch16_224_imagenet-1k_300ep_bs512.csv\n",
      "swin_base_patch4_window7_224_imagenet-1k_300ep_bs128.csv\n",
      "vit_large_patch16_224_imagenet-1k_300ep_bs128.csv\n",
      "efficientnet_v2_s_imagenet-1k_300ep_bs512.csv\n",
      "tiny_vit_21m_224_imagenet-1k_300ep_bs512.csv\n",
      "resnet152_imagenet-1k_300ep_bs128.csv\n",
      "regnet_y_16gf_imagenet-1k_300ep_bs128.csv\n",
      "beit_large_patch16_224_imagenet-1k_300ep_bs128.csv\n",
      "convnext_large_imagenet-1k_300ep_bs256.csv\n",
      "swin_base_patch4_window7_224_imagenet-1k_300ep_bs256.csv\n",
      "swin_base_patch4_window7_224_imagenet-1k_300ep_bs512.csv\n",
      "tiny_vit_21m_224_imagenet-1k_300ep_bs128.csv\n",
      "vit_base_patch16_224_imagenet-1k_300ep_bs512.csv\n",
      "resnet152_imagenet-1k_300ep_bs256.csv\n",
      "deit_large_patch16_224_imagenet-1k_300ep_bs128.csv\n",
      "resnet101_imagenet-1k_300ep_bs256.csv\n",
      "convnext_xlarge_imagenet-1k_300ep_bs512.csv\n",
      "beit_base_patch16_224_imagenet-1k_300ep_bs512.csv\n",
      "regnet_y_16gf_imagenet-1k_300ep_bs256.csv\n",
      "resnet152_imagenet-1k_300ep_bs512.csv\n",
      "deit_base_patch16_224_imagenet-1k_300ep_bs128.csv\n",
      "vit_base_patch16_224_imagenet-1k_300ep_bs256.csv\n",
      "vit_large_patch16_224_imagenet-1k_300ep_bs512.csv\n",
      "vit_huge_patch14_224_imagenet-1k_300ep_bs512.csv\n",
      "efficientnet_v2_s_imagenet-1k_300ep_bs128.csv\n",
      "swin_large_patch4_window7_224_imagenet-1k_300ep_bs512.csv\n",
      "vit_huge_patch14_224_imagenet-1k_300ep_bs128.csv\n",
      "swin_large_patch4_window7_224_imagenet-1k_300ep_bs128.csv\n",
      "efficientnet_b3_imagenet-1k_300ep_bs256.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the folder\n",
    "folder_path = 'A100-80GB/eco2ai_logs'\n",
    "\n",
    "# List all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        print(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76b86c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       model_name  bs128  bs256  bs512\n",
      "0                 efficientnet_b3   True   True   True\n",
      "1                   convnext_base   True   True  False\n",
      "2           deit_base_patch16_224   True   True   True\n",
      "3                 convnext_xlarge   True  False  False\n",
      "4           vit_large_patch16_224   True   True  False\n",
      "5                tiny_vit_21m_224   True   True   True\n",
      "6                       resnet101   True   True   True\n",
      "7           beit_base_patch16_224   True   True   True\n",
      "8          beit_large_patch16_224   True   True  False\n",
      "9                  convnext_large   True   True  False\n",
      "10           vit_base_patch16_224   True   True   True\n",
      "11   swin_base_patch4_window7_224   True   True   True\n",
      "12                      resnet152   True   True   True\n",
      "13           vit_huge_patch14_224   True  False  False\n",
      "14  swin_large_patch4_window7_224   True  False  False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Path to the folder containing the CSV logs\n",
    "folder_path = 'A100-80GB/eco2ai_logs'\n",
    "\n",
    "# Collect all CSV filenames in the folder\n",
    "filenames = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Dictionary to collect model info\n",
    "model_dict = {}\n",
    "\n",
    "# Process each filename\n",
    "for fname in filenames:\n",
    "    match = re.match(r'(.+?)_imagenet-1k_300ep_bs(\\d+)\\.csv', fname)\n",
    "    if match:\n",
    "        model_name, bs = match.groups()\n",
    "        if model_name not in model_dict:\n",
    "            model_dict[model_name] = {'bs128': False, 'bs256': False, 'bs512': False}\n",
    "        model_dict[model_name][f'bs{bs}'] = True\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(model_dict, orient='index').reset_index()\n",
    "df.rename(columns={'index': 'model_name'}, inplace=True)\n",
    "\n",
    "# Display DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df.to_csv('model_batch_size_availability.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec5b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files in eco2ai_logs that would be deleted (no matching model folder):\n",
      "beit_large_patch16_224_imagenet-1k_300ep_bs512.csv\n",
      "convnext_base_imagenet-1k_300ep_bs512.csv\n",
      "convnext_large_imagenet-1k_300ep_bs512.csv\n",
      "convnext_xlarge_imagenet-1k_300ep_bs256.csv\n",
      "convnext_xlarge_imagenet-1k_300ep_bs512.csv\n",
      "swin_large_patch4_window7_224_imagenet-1k_300ep_bs256.csv\n",
      "swin_large_patch4_window7_224_imagenet-1k_300ep_bs512.csv\n",
      "vit_huge_patch14_224_imagenet-1k_300ep_bs256.csv\n",
      "vit_huge_patch14_224_imagenet-1k_300ep_bs512.csv\n",
      "vit_large_patch16_224_imagenet-1k_300ep_bs512.csv\n",
      "\n",
      "Total files that would be deleted: 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths\n",
    "models_path = 'A100-80GB/models'\n",
    "logs_path = 'A100-80GB/eco2ai_logs'\n",
    "\n",
    "# Step 1: Get all experiment names (folders) in models/\n",
    "model_experiments = {name for name in os.listdir(models_path) if os.path.isdir(os.path.join(models_path, name))}\n",
    "\n",
    "# Step 2: Identify extra CSV files in eco2ai_logs\n",
    "files_to_delete = []\n",
    "for filename in os.listdir(logs_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        exp_name = filename.replace('.csv', '')\n",
    "        if exp_name not in model_experiments:\n",
    "            files_to_delete.append(filename)\n",
    "\n",
    "# Step 3: Print results\n",
    "print(\"CSV files in eco2ai_logs that would be deleted (no matching model folder):\")\n",
    "for fname in sorted(files_to_delete):\n",
    "    print(fname)\n",
    "\n",
    "print(f\"\\nTotal files that would be deleted: {len(files_to_delete)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d918958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV updated successfully.\n",
      "Deleted file: A100-80GB/eco2ai_logs/beit_large_patch16_224_imagenet-1k_300ep_bs512.csv\n",
      "Deleted file: A100-80GB/eco2ai_logs/convnext_base_imagenet-1k_300ep_bs512.csv\n",
      "Deleted file: A100-80GB/eco2ai_logs/convnext_large_imagenet-1k_300ep_bs512.csv\n",
      "Deleted file: A100-80GB/eco2ai_logs/convnext_xlarge_imagenet-1k_300ep_bs256.csv\n",
      "Deleted file: A100-80GB/eco2ai_logs/convnext_xlarge_imagenet-1k_300ep_bs512.csv\n",
      "Deleted file: A100-80GB/eco2ai_logs/swin_large_patch4_window7_224_imagenet-1k_300ep_bs256.csv\n",
      "Deleted file: A100-80GB/eco2ai_logs/swin_large_patch4_window7_224_imagenet-1k_300ep_bs512.csv\n",
      "Deleted file: A100-80GB/eco2ai_logs/vit_huge_patch14_224_imagenet-1k_300ep_bs256.csv\n",
      "Deleted file: A100-80GB/eco2ai_logs/vit_huge_patch14_224_imagenet-1k_300ep_bs512.csv\n",
      "Deleted file: A100-80GB/eco2ai_logs/vit_large_patch16_224_imagenet-1k_300ep_bs512.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "csv_path = 'model_batch_size_availability.csv'\n",
    "eco2ai_logs_path = 'A100-80GB/eco2ai_logs'\n",
    "\n",
    "# Load the existing CSV into a DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# List of failed experiment filenames\n",
    "failed_exps = [\n",
    "    \"beit_large_patch16_224_imagenet-1k_300ep_bs512.csv\",\n",
    "    \"convnext_base_imagenet-1k_300ep_bs512.csv\",\n",
    "    \"convnext_large_imagenet-1k_300ep_bs512.csv\",\n",
    "    \"convnext_xlarge_imagenet-1k_300ep_bs256.csv\",\n",
    "    \"convnext_xlarge_imagenet-1k_300ep_bs512.csv\",\n",
    "    \"swin_large_patch4_window7_224_imagenet-1k_300ep_bs256.csv\",\n",
    "    \"swin_large_patch4_window7_224_imagenet-1k_300ep_bs512.csv\",\n",
    "    \"vit_huge_patch14_224_imagenet-1k_300ep_bs256.csv\",\n",
    "    \"vit_huge_patch14_224_imagenet-1k_300ep_bs512.csv\",\n",
    "    \"vit_large_patch16_224_imagenet-1k_300ep_bs512.csv\",\n",
    "]\n",
    "\n",
    "# Update the DataFrame based on failed experiments\n",
    "for exp in failed_exps:\n",
    "    parts = exp.replace('.csv', '').split('_imagenet-1k_300ep_bs')\n",
    "    model_name, bs = parts[0], parts[1]\n",
    "    col = f'bs{bs}'\n",
    "    if model_name in df['model_name'].values and col in df.columns:\n",
    "        df.loc[df['model_name'] == model_name, col] = False\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"CSV updated successfully.\")\n",
    "\n",
    "# Delete the failed experiment files from eco2ai_logs\n",
    "for exp in failed_exps:\n",
    "    file_path = os.path.join(eco2ai_logs_path, exp)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File not found (skipped): {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9946c430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sorted epoch + eval_top1 information to next_epoch_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Root path to models\n",
    "models_path = '/home/escade/ESCADE/Cluster_results/A100-80GB/models'\n",
    "\n",
    "# List to hold experiment data\n",
    "epoch_info = []\n",
    "\n",
    "# Iterate over all experiment directories\n",
    "for exp_folder in os.listdir(models_path):\n",
    "    exp_path = os.path.join(models_path, exp_folder, exp_folder)\n",
    "\n",
    "    if not os.path.isdir(exp_path):\n",
    "        continue\n",
    "\n",
    "    summary_file = os.path.join(exp_path, 'summary.csv')\n",
    "\n",
    "    if not os.path.isfile(summary_file):\n",
    "        print(f\"summary.csv not found in: {exp_folder}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(summary_file)\n",
    "\n",
    "        if df.empty or 'epoch' not in df.columns or 'eval_top1' not in df.columns:\n",
    "            print(f\"Missing required columns in: {exp_folder}\")\n",
    "            continue\n",
    "\n",
    "        last_row = df.iloc[-1]\n",
    "        epoch_number = int(last_row['epoch']) + 1\n",
    "        eval_top1 = last_row['eval_top1']\n",
    "\n",
    "        epoch_info.append({\n",
    "            'exp_name': exp_folder,\n",
    "            'epoch': epoch_number,\n",
    "            'eval_top1': eval_top1\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {summary_file}: {e}\")\n",
    "\n",
    "# Create DataFrame and sort alphabetically by exp_name\n",
    "output_df = pd.DataFrame(epoch_info).sort_values(by='exp_name')\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv('exp_summary.csv', index=False)\n",
    "print(\"Saved sorted epoch + eval_top1 information to next_epoch_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2b685ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged summary (metrics + energy) to exp_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Part 1: Read summary.csv files from models ===\n",
    "models_path = '/home/escade/ESCADE/Cluster_results/A100-80GB/models'\n",
    "epoch_info = []\n",
    "\n",
    "for exp_folder in os.listdir(models_path):\n",
    "    exp_path = os.path.join(models_path, exp_folder, exp_folder)\n",
    "    if not os.path.isdir(exp_path):\n",
    "        continue\n",
    "\n",
    "    summary_file = os.path.join(exp_path, 'summary.csv')\n",
    "    if not os.path.isfile(summary_file):\n",
    "        print(f\"summary.csv not found in: {exp_folder}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(summary_file)\n",
    "        if df.empty or 'epoch' not in df.columns or 'eval_top1' not in df.columns:\n",
    "            print(f\"Missing required columns in: {exp_folder}\")\n",
    "            continue\n",
    "\n",
    "        last_row = df.iloc[-1]\n",
    "        epoch_number = int(last_row['epoch']) + 1\n",
    "        eval_top1 = last_row['eval_top1']\n",
    "\n",
    "        epoch_info.append({\n",
    "            'exp_name': exp_folder,\n",
    "            'epoch': epoch_number,\n",
    "            'eval_top1': eval_top1\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {summary_file}: {e}\")\n",
    "\n",
    "model_df = pd.DataFrame(epoch_info)\n",
    "\n",
    "# === Part 2: Read energy info from eco2ai_logs ===\n",
    "eco2ai_logs_path = 'A100-80GB/eco2ai_logs'\n",
    "energy_info = []\n",
    "\n",
    "for filename in os.listdir(eco2ai_logs_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        exp_name = filename.replace('.csv', '')\n",
    "        file_path = os.path.join(eco2ai_logs_path, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.empty or 'duration(s)' not in df.columns or 'power_consumption(kWh)' not in df.columns:\n",
    "                print(f\"Missing expected columns in: {filename}\")\n",
    "                continue\n",
    "\n",
    "            last_row = df.iloc[-1]\n",
    "            energy_info.append({\n",
    "                'exp_name': exp_name,\n",
    "                'duration(s)': last_row['duration(s)'],\n",
    "                'power_consumption(kWh)': last_row['power_consumption(kWh)']\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "energy_df = pd.DataFrame(energy_info)\n",
    "\n",
    "# === Part 3: Merge and save ===\n",
    "merged_df = pd.merge(model_df, energy_df, on='exp_name', how='left')\n",
    "merged_df = merged_df.sort_values(by='exp_name')\n",
    "merged_df.to_csv('exp_summary.csv', index=False)\n",
    "\n",
    "print(\"Saved merged summary (metrics + energy) to exp_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082186ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Top 5 Minimum Duration Experiments:\n",
      "\n",
      "                              Experiment Name  Accuracy (Top-1)  Energy (kWh)  Duration (s)\n",
      " vit_huge_patch14_224_imagenet-1k_300ep_bs128            56.116     22.397888 204628.812999\n",
      "deit_base_patch16_224_imagenet-1k_300ep_bs256            64.276     21.751035 259150.487861\n",
      "      efficientnet_b3_imagenet-1k_300ep_bs128            75.518     19.936000 259150.554763\n",
      "        convnext_base_imagenet-1k_300ep_bs128            69.400     18.470686 259160.490635\n",
      "      convnext_xlarge_imagenet-1k_300ep_bs128            68.524     26.761583 259160.493673\n",
      "\n",
      "⏳ Top 5 Maximum Duration Experiments:\n",
      "\n",
      "                              Experiment Name  Accuracy (Top-1)  Energy (kWh)  Duration (s)\n",
      "       convnext_large_imagenet-1k_300ep_bs128            72.064     22.936648 259181.033674\n",
      " vit_base_patch16_224_imagenet-1k_300ep_bs512            63.632     15.802139 259180.587630\n",
      " vit_base_patch16_224_imagenet-1k_300ep_bs128            64.134     18.365596 259180.552574\n",
      "deit_base_patch16_224_imagenet-1k_300ep_bs512            64.174     22.671158 259180.539638\n",
      "            resnet152_imagenet-1k_300ep_bs256            74.888     24.039883 259180.519368\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with duration and energy info\n",
    "file_path = 'exp_summary.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows where duration is missing\n",
    "df = df.dropna(subset=['duration(s)'])\n",
    "\n",
    "# Select relevant columns\n",
    "columns = ['exp_name', 'eval_top1', 'power_consumption(kWh)', 'duration(s)']\n",
    "\n",
    "# Get top 5 experiments with min and max durations\n",
    "min_duration_table = df.nsmallest(5, 'duration(s)')[columns].copy()\n",
    "max_duration_table = df.nlargest(5, 'duration(s)')[columns].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "min_duration_table.columns = ['Experiment Name', 'Accuracy (Top-1)', 'Energy (kWh)', 'Duration (s)']\n",
    "max_duration_table.columns = ['Experiment Name', 'Accuracy (Top-1)', 'Energy (kWh)', 'Duration (s)']\n",
    "\n",
    "# Print results\n",
    "print(\"⏱️ Top 5 Minimum Duration Experiments:\\n\")\n",
    "print(min_duration_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n⏳ Top 5 Maximum Duration Experiments:\\n\")\n",
    "print(max_duration_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49608a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating efficientnet_b3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::silu_ encountered 78 time(s)\n",
      "Unsupported operator aten::mean encountered 26 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 26 time(s)\n",
      "Unsupported operator aten::mul encountered 26 time(s)\n",
      "Unsupported operator aten::add encountered 19 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating convnext_base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::gelu encountered 36 time(s)\n",
      "Unsupported operator aten::mul encountered 36 time(s)\n",
      "Unsupported operator aten::add encountered 36 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating deit_base_patch16_224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 25 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "blocks.0.attn.attn_drop, blocks.1.attn.attn_drop, blocks.10.attn.attn_drop, blocks.11.attn.attn_drop, blocks.2.attn.attn_drop, blocks.3.attn.attn_drop, blocks.4.attn.attn_drop, blocks.5.attn.attn_drop, blocks.6.attn.attn_drop, blocks.7.attn.attn_drop, blocks.8.attn.attn_drop, blocks.9.attn.attn_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating convnext_xlarge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::gelu encountered 36 time(s)\n",
      "Unsupported operator aten::mul encountered 36 time(s)\n",
      "Unsupported operator aten::add encountered 36 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating vit_large_patch16_224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 49 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 24 time(s)\n",
      "Unsupported operator aten::gelu encountered 24 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "blocks.0.attn.attn_drop, blocks.1.attn.attn_drop, blocks.10.attn.attn_drop, blocks.11.attn.attn_drop, blocks.12.attn.attn_drop, blocks.13.attn.attn_drop, blocks.14.attn.attn_drop, blocks.15.attn.attn_drop, blocks.16.attn.attn_drop, blocks.17.attn.attn_drop, blocks.18.attn.attn_drop, blocks.19.attn.attn_drop, blocks.2.attn.attn_drop, blocks.20.attn.attn_drop, blocks.21.attn.attn_drop, blocks.22.attn.attn_drop, blocks.23.attn.attn_drop, blocks.3.attn.attn_drop, blocks.4.attn.attn_drop, blocks.5.attn.attn_drop, blocks.6.attn.attn_drop, blocks.7.attn.attn_drop, blocks.8.attn.attn_drop, blocks.9.attn.attn_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating tiny_vit_21m_224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::gelu encountered 23 time(s)\n",
      "Unsupported operator aten::add_ encountered 2 time(s)\n",
      "Unsupported operator aten::mul encountered 14 time(s)\n",
      "Unsupported operator aten::rsub encountered 4 time(s)\n",
      "Unsupported operator aten::add encountered 24 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 10 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "stages.0.blocks.1.drop_path, stages.1.blocks.0.drop_path1, stages.1.blocks.0.drop_path2, stages.1.blocks.1.drop_path1, stages.1.blocks.1.drop_path2, stages.2.blocks.0.drop_path1, stages.2.blocks.0.drop_path2, stages.2.blocks.1.drop_path1, stages.2.blocks.1.drop_path2, stages.2.blocks.2.drop_path1, stages.2.blocks.2.drop_path2, stages.2.blocks.3.drop_path1, stages.2.blocks.3.drop_path2, stages.2.blocks.4.drop_path1, stages.2.blocks.4.drop_path2, stages.2.blocks.5.drop_path1, stages.2.blocks.5.drop_path2, stages.3.blocks.0.drop_path1, stages.3.blocks.0.drop_path2, stages.3.blocks.1.drop_path1, stages.3.blocks.1.drop_path2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating resnet101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "Unsupported operator aten::add_ encountered 33 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating beit_base_patch16_224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::mul encountered 24 time(s)\n",
      "Unsupported operator aten::add encountered 24 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "blocks.0.attn.attn_drop, blocks.0.attn.qkv, blocks.1.attn.attn_drop, blocks.1.attn.qkv, blocks.10.attn.attn_drop, blocks.10.attn.qkv, blocks.11.attn.attn_drop, blocks.11.attn.qkv, blocks.2.attn.attn_drop, blocks.2.attn.qkv, blocks.3.attn.attn_drop, blocks.3.attn.qkv, blocks.4.attn.attn_drop, blocks.4.attn.qkv, blocks.5.attn.attn_drop, blocks.5.attn.qkv, blocks.6.attn.attn_drop, blocks.6.attn.qkv, blocks.7.attn.attn_drop, blocks.7.attn.qkv, blocks.8.attn.attn_drop, blocks.8.attn.qkv, blocks.9.attn.attn_drop, blocks.9.attn.qkv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating beit_large_patch16_224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::scaled_dot_product_attention encountered 24 time(s)\n",
      "Unsupported operator aten::mul encountered 48 time(s)\n",
      "Unsupported operator aten::add encountered 48 time(s)\n",
      "Unsupported operator aten::gelu encountered 24 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "blocks.0.attn.attn_drop, blocks.0.attn.qkv, blocks.1.attn.attn_drop, blocks.1.attn.qkv, blocks.10.attn.attn_drop, blocks.10.attn.qkv, blocks.11.attn.attn_drop, blocks.11.attn.qkv, blocks.12.attn.attn_drop, blocks.12.attn.qkv, blocks.13.attn.attn_drop, blocks.13.attn.qkv, blocks.14.attn.attn_drop, blocks.14.attn.qkv, blocks.15.attn.attn_drop, blocks.15.attn.qkv, blocks.16.attn.attn_drop, blocks.16.attn.qkv, blocks.17.attn.attn_drop, blocks.17.attn.qkv, blocks.18.attn.attn_drop, blocks.18.attn.qkv, blocks.19.attn.attn_drop, blocks.19.attn.qkv, blocks.2.attn.attn_drop, blocks.2.attn.qkv, blocks.20.attn.attn_drop, blocks.20.attn.qkv, blocks.21.attn.attn_drop, blocks.21.attn.qkv, blocks.22.attn.attn_drop, blocks.22.attn.qkv, blocks.23.attn.attn_drop, blocks.23.attn.qkv, blocks.3.attn.attn_drop, blocks.3.attn.qkv, blocks.4.attn.attn_drop, blocks.4.attn.qkv, blocks.5.attn.attn_drop, blocks.5.attn.qkv, blocks.6.attn.attn_drop, blocks.6.attn.qkv, blocks.7.attn.attn_drop, blocks.7.attn.qkv, blocks.8.attn.attn_drop, blocks.8.attn.qkv, blocks.9.attn.attn_drop, blocks.9.attn.qkv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating convnext_large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::gelu encountered 36 time(s)\n",
      "Unsupported operator aten::mul encountered 36 time(s)\n",
      "Unsupported operator aten::add encountered 36 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating vit_base_patch16_224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 25 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "blocks.0.attn.attn_drop, blocks.1.attn.attn_drop, blocks.10.attn.attn_drop, blocks.11.attn.attn_drop, blocks.2.attn.attn_drop, blocks.3.attn.attn_drop, blocks.4.attn.attn_drop, blocks.5.attn.attn_drop, blocks.6.attn.attn_drop, blocks.7.attn.attn_drop, blocks.8.attn.attn_drop, blocks.9.attn.attn_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating swin_base_patch4_window7_224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::rsub encountered 48 time(s)\n",
      "Unsupported operator aten::pad encountered 27 time(s)\n",
      "Unsupported operator aten::mul encountered 24 time(s)\n",
      "Unsupported operator aten::add encountered 83 time(s)\n",
      "Unsupported operator aten::softmax encountered 24 time(s)\n",
      "Unsupported operator aten::gelu encountered 24 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "layers.0.blocks.1.drop_path1, layers.0.blocks.1.drop_path2, layers.1.blocks.0.drop_path1, layers.1.blocks.0.drop_path2, layers.1.blocks.1.drop_path1, layers.1.blocks.1.drop_path2, layers.2.blocks.0.drop_path1, layers.2.blocks.0.drop_path2, layers.2.blocks.1.drop_path1, layers.2.blocks.1.drop_path2, layers.2.blocks.10.drop_path1, layers.2.blocks.10.drop_path2, layers.2.blocks.11.drop_path1, layers.2.blocks.11.drop_path2, layers.2.blocks.12.drop_path1, layers.2.blocks.12.drop_path2, layers.2.blocks.13.drop_path1, layers.2.blocks.13.drop_path2, layers.2.blocks.14.drop_path1, layers.2.blocks.14.drop_path2, layers.2.blocks.15.drop_path1, layers.2.blocks.15.drop_path2, layers.2.blocks.16.drop_path1, layers.2.blocks.16.drop_path2, layers.2.blocks.17.drop_path1, layers.2.blocks.17.drop_path2, layers.2.blocks.2.drop_path1, layers.2.blocks.2.drop_path2, layers.2.blocks.3.drop_path1, layers.2.blocks.3.drop_path2, layers.2.blocks.4.drop_path1, layers.2.blocks.4.drop_path2, layers.2.blocks.5.drop_path1, layers.2.blocks.5.drop_path2, layers.2.blocks.6.drop_path1, layers.2.blocks.6.drop_path2, layers.2.blocks.7.drop_path1, layers.2.blocks.7.drop_path2, layers.2.blocks.8.drop_path1, layers.2.blocks.8.drop_path2, layers.2.blocks.9.drop_path1, layers.2.blocks.9.drop_path2, layers.3.blocks.0.drop_path1, layers.3.blocks.0.drop_path2, layers.3.blocks.1.drop_path1, layers.3.blocks.1.drop_path2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating resnet152...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "Unsupported operator aten::add_ encountered 50 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating vit_huge_patch14_224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 65 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 32 time(s)\n",
      "Unsupported operator aten::gelu encountered 32 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "blocks.0.attn.attn_drop, blocks.1.attn.attn_drop, blocks.10.attn.attn_drop, blocks.11.attn.attn_drop, blocks.12.attn.attn_drop, blocks.13.attn.attn_drop, blocks.14.attn.attn_drop, blocks.15.attn.attn_drop, blocks.16.attn.attn_drop, blocks.17.attn.attn_drop, blocks.18.attn.attn_drop, blocks.19.attn.attn_drop, blocks.2.attn.attn_drop, blocks.20.attn.attn_drop, blocks.21.attn.attn_drop, blocks.22.attn.attn_drop, blocks.23.attn.attn_drop, blocks.24.attn.attn_drop, blocks.25.attn.attn_drop, blocks.26.attn.attn_drop, blocks.27.attn.attn_drop, blocks.28.attn.attn_drop, blocks.29.attn.attn_drop, blocks.3.attn.attn_drop, blocks.30.attn.attn_drop, blocks.31.attn.attn_drop, blocks.4.attn.attn_drop, blocks.5.attn.attn_drop, blocks.6.attn.attn_drop, blocks.7.attn.attn_drop, blocks.8.attn.attn_drop, blocks.9.attn.attn_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating swin_large_patch4_window7_224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::rsub encountered 48 time(s)\n",
      "Unsupported operator aten::pad encountered 27 time(s)\n",
      "Unsupported operator aten::mul encountered 24 time(s)\n",
      "Unsupported operator aten::add encountered 83 time(s)\n",
      "Unsupported operator aten::softmax encountered 24 time(s)\n",
      "Unsupported operator aten::gelu encountered 24 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "layers.0.blocks.1.drop_path1, layers.0.blocks.1.drop_path2, layers.1.blocks.0.drop_path1, layers.1.blocks.0.drop_path2, layers.1.blocks.1.drop_path1, layers.1.blocks.1.drop_path2, layers.2.blocks.0.drop_path1, layers.2.blocks.0.drop_path2, layers.2.blocks.1.drop_path1, layers.2.blocks.1.drop_path2, layers.2.blocks.10.drop_path1, layers.2.blocks.10.drop_path2, layers.2.blocks.11.drop_path1, layers.2.blocks.11.drop_path2, layers.2.blocks.12.drop_path1, layers.2.blocks.12.drop_path2, layers.2.blocks.13.drop_path1, layers.2.blocks.13.drop_path2, layers.2.blocks.14.drop_path1, layers.2.blocks.14.drop_path2, layers.2.blocks.15.drop_path1, layers.2.blocks.15.drop_path2, layers.2.blocks.16.drop_path1, layers.2.blocks.16.drop_path2, layers.2.blocks.17.drop_path1, layers.2.blocks.17.drop_path2, layers.2.blocks.2.drop_path1, layers.2.blocks.2.drop_path2, layers.2.blocks.3.drop_path1, layers.2.blocks.3.drop_path2, layers.2.blocks.4.drop_path1, layers.2.blocks.4.drop_path2, layers.2.blocks.5.drop_path1, layers.2.blocks.5.drop_path2, layers.2.blocks.6.drop_path1, layers.2.blocks.6.drop_path2, layers.2.blocks.7.drop_path1, layers.2.blocks.7.drop_path2, layers.2.blocks.8.drop_path1, layers.2.blocks.8.drop_path2, layers.2.blocks.9.drop_path1, layers.2.blocks.9.drop_path2, layers.3.blocks.0.drop_path1, layers.3.blocks.0.drop_path2, layers.3.blocks.1.drop_path1, layers.3.blocks.1.drop_path2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to model_flops_params.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from timm import create_model\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "import pandas as pd\n",
    "\n",
    "# Define models and input sizes\n",
    "model_inputs = {\n",
    "    \"efficientnet_b3\": (3, 224, 224),\n",
    "    \"convnext_base\": (3, 224, 224),\n",
    "    \"deit_base_patch16_224\": (3, 224, 224),\n",
    "    \"convnext_xlarge\": (3, 224, 224),\n",
    "    \"vit_large_patch16_224\": (3, 224, 224),\n",
    "    \"tiny_vit_21m_224\": (3, 224, 224),\n",
    "    \"resnet101\": (3, 224, 224),\n",
    "    \"beit_base_patch16_224\": (3, 224, 224),\n",
    "    \"beit_large_patch16_224\": (3, 224, 224),\n",
    "    \"convnext_large\": (3, 224, 224),\n",
    "    \"vit_base_patch16_224\": (3, 224, 224),\n",
    "    \"swin_base_patch4_window7_224\": (3, 224, 224),\n",
    "    \"resnet152\": (3, 224, 224),\n",
    "    \"vit_huge_patch14_224\": (3, 224, 224),\n",
    "    \"swin_large_patch4_window7_224\": (3, 224, 224),\n",
    "}\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for model_name, input_shape in model_inputs.items():\n",
    "    try:\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        model = create_model(model_name, pretrained=False).to(device).eval()\n",
    "        input_tensor = torch.randn(1, *input_shape).to(device)\n",
    "\n",
    "        # FLOPs calculation\n",
    "        flops = FlopCountAnalysis(model, input_tensor)\n",
    "        forward_flops = flops.total() / 1e9  # Convert to GFLOPs\n",
    "        training_flops = forward_flops * 2   # Estimated training FLOPs (forward + backward only)\n",
    "\n",
    "        # Params calculation\n",
    "        num_params = sum(p.numel() for p in model.parameters()) / 1e6  # Convert to Millions\n",
    "\n",
    "        results.append((model_name, round(forward_flops, 3), round(training_flops, 3), round(num_params, 2)))\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append((model_name, \"❌ Error\", \"❌ Error\", str(e)))\n",
    "\n",
    "# Save to CSV\n",
    "# Flops in GFLOPs, Params in Millions\n",
    "df = pd.DataFrame(results, columns=[\"model\", \"forward_flops\", \"training_flops\", \"params\"])\n",
    "df.to_csv(\"model_flops_params.csv\", index=False)\n",
    "print(\"✅ Saved to model_flops_params.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbb3db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated exp_summary.csv with model_name column.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the existing CSV\n",
    "file_path = 'exp_summary.csv'\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract model_name from exp_name\n",
    "df['model_name'] = df['exp_name'].str.extract(r'^(.*?)_imagenet-1k_300ep_bs')\n",
    "\n",
    "# Optional: move model_name to the front\n",
    "cols = ['model_name'] + [col for col in df.columns if col != 'model_name']\n",
    "df = df[cols]\n",
    "\n",
    "# Save back to the same file\n",
    "df.to_csv(file_path, index=False)\n",
    "print(\"Updated exp_summary.csv with model_name column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f31ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs and params added in training_energy.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSVs # Change the path to your actual file paths\n",
    "flops_df = pd.read_csv(\"model_flops_params.csv\")\n",
    "energy_df = pd.read_csv(\"exp_summary.csv\")\n",
    "\n",
    "# Select only the columns to append (exclude duplicate 'model')\n",
    "flops_data = flops_df.set_index(\"model\")[[\"forward_flops\", \"training_flops\", \"params\"]]\n",
    "\n",
    "# Match by model and append columns\n",
    "for col in flops_data.columns:\n",
    "    energy_df[col] = energy_df[\"model_name\"].map(flops_data[col])\n",
    "\n",
    "# Save result\n",
    "energy_df.to_csv(\"exp_summary.csv\", index=False)\n",
    "\n",
    "print(\"FLOPs and params added in training_energy.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601b0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "929c4d85",
   "metadata": {},
   "source": [
    "## SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09e172e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM values calculated and saved to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file from the specified path\n",
    "file_path = 'exp_summary.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define constants\n",
    "alpha = 5\n",
    "beta = 5\n",
    "\n",
    "# Normalize accuracy (0–100 → 0–1)\n",
    "accuracy_normalized = df['eval_top1'] / 100\n",
    "\n",
    "# Calculate SAM\n",
    "df['SAM'] = beta * (accuracy_normalized ** alpha) / np.log10(df['power_consumption(kWh)'])\n",
    "\n",
    "# Save the updated DataFrame back to the same file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"SAM values calculated and saved to the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e78eb2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔽 Top 5 Minimum SAM Entries:\n",
      "\n",
      "                               Experiment Name  Accuracy (Top-1)  Energy (kWh)      SAM  training_flops  params  throughput\n",
      "  vit_huge_patch14_224_imagenet-1k_300ep_bs128            56.116     22.397888 0.206064         323.980  630.76  137.740495\n",
      " vit_large_patch16_224_imagenet-1k_300ep_bs256            61.362     27.846182 0.301072         119.393  304.33  350.990436\n",
      " vit_large_patch16_224_imagenet-1k_300ep_bs128            61.868     28.084009 0.312892         119.393  304.33  336.146911\n",
      "beit_large_patch16_224_imagenet-1k_300ep_bs256            62.846     28.000986 0.338718         119.391  304.43  311.442106\n",
      "beit_large_patch16_224_imagenet-1k_300ep_bs128            63.046     27.258405 0.346940         119.391  304.43  286.713549\n",
      "\n",
      "🔼 Top 5 Maximum SAM Entries:\n",
      "\n",
      "                         Experiment Name  Accuracy (Top-1)  Energy (kWh)      SAM  training_flops  params  throughput\n",
      "tiny_vit_21m_224_imagenet-1k_300ep_bs128            76.354     20.675261 0.986403           8.151   33.22  726.670331\n",
      "tiny_vit_21m_224_imagenet-1k_300ep_bs256            76.122     20.593612 0.972779           8.151   33.22  766.187590\n",
      " efficientnet_b3_imagenet-1k_300ep_bs128            75.518     19.936000 0.944931           1.978   12.23  504.259133\n",
      "tiny_vit_21m_224_imagenet-1k_300ep_bs512            75.756     21.213653 0.940395           8.151   33.22  766.246709\n",
      " efficientnet_b3_imagenet-1k_300ep_bs256            75.304     20.211348 0.927367           1.978   12.23  509.183224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV\n",
    "file_path = 'exp_summary.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select relevant columns\n",
    "columns = ['exp_name', 'eval_top1', 'power_consumption(kWh)', 'SAM', 'training_flops', 'params', 'throughput']\n",
    "\n",
    "# Get top 5 min and max SAM_watt entries\n",
    "min_sam_table = df.nsmallest(5, 'SAM')[columns].copy()\n",
    "max_sam_table = df.nlargest(5, 'SAM')[columns].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "min_sam_table.columns = ['Experiment Name', 'Accuracy (Top-1)', 'Energy (kWh)', 'SAM' , 'training_flops', 'params', 'throughput']\n",
    "max_sam_table.columns = ['Experiment Name', 'Accuracy (Top-1)', 'Energy (kWh)', 'SAM' , 'training_flops', 'params', 'throughput']\n",
    "\n",
    "# Print the tables\n",
    "print(\"🔽 Top 5 Minimum SAM Entries:\\n\")\n",
    "print(min_sam_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n🔼 Top 5 Maximum SAM Entries:\\n\")\n",
    "print(max_sam_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5507fdf",
   "metadata": {},
   "source": [
    "# ESPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7c607cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ESPM, EF, and Throughput calculated using 400 gCO₂/kWh (converted to kg) and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'exp_summary.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Constants\n",
    "alpha = 5\n",
    "num_training_samples = 1281167  # Imagenet-1k\n",
    "carbon_intensity_kg = 0.35  # kgCO₂/kWh\n",
    "\n",
    "pue = 1.0  # Power Usage Effectiveness (assumed value)\n",
    "\n",
    "# Normalize accuracy (0–100 → 0–1)\n",
    "accuracy_normalized = df['eval_top1'] / 100\n",
    "\n",
    "# Throughput = total samples / duration\n",
    "df['throughput'] = (num_training_samples * df['epoch']) / df['duration(s)']\n",
    "\n",
    "# Convert GFLOPs → FLOPs and Million Params → actual count\n",
    "training_flops_actual = df['training_flops'] * 1e9\n",
    "params_actual = df['params'] * 1e6\n",
    "\n",
    "# Efficiency Factor (EF)\n",
    "df['ef'] = df['throughput'] / np.log10(training_flops_actual * params_actual)\n",
    "\n",
    "\n",
    "# ESPM using converted carbon intensity\n",
    "df['espm'] = (accuracy_normalized ** alpha * df['ef']) / np.log10(\n",
    "    df['power_consumption(kWh)'] * pue * carbon_intensity_kg\n",
    ")\n",
    "\n",
    "# Save updated file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"✅ ESPM, EF, and Throughput calculated using 400 gCO₂/kWh (converted to kg) and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04d4c6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔽 Top 5 Minimum ESPM Entries:\n",
      "\n",
      "                               Experiment Name  Accuracy (Top-1)  Energy (kWh)     ESPM  training_flops  params  throughput\n",
      "  vit_huge_patch14_224_imagenet-1k_300ep_bs128            56.116     22.397888 0.421994         323.980  630.76  137.740495\n",
      "beit_large_patch16_224_imagenet-1k_300ep_bs128            63.046     27.258405 1.490466         119.391  304.43  286.713549\n",
      " vit_large_patch16_224_imagenet-1k_300ep_bs128            61.868     28.084009 1.569427         119.393  304.33  336.146911\n",
      "beit_large_patch16_224_imagenet-1k_300ep_bs256            62.846     28.000986 1.574734         119.391  304.43  311.442106\n",
      " vit_large_patch16_224_imagenet-1k_300ep_bs256            61.362     27.846182 1.578678         119.393  304.33  350.990436\n",
      "\n",
      "🔼 Top 5 Maximum ESPM Entries:\n",
      "\n",
      "                         Experiment Name  Accuracy (Top-1)  Energy (kWh)      ESPM  training_flops  params  throughput\n",
      "tiny_vit_21m_224_imagenet-1k_300ep_bs256            76.122     20.593612 13.095959           8.151   33.22  766.187590\n",
      "tiny_vit_21m_224_imagenet-1k_300ep_bs512            75.756     21.213653 12.595954           8.151   33.22  766.246709\n",
      "tiny_vit_21m_224_imagenet-1k_300ep_bs128            76.354     20.675261 12.585732           8.151   33.22  726.670331\n",
      "       resnet101_imagenet-1k_300ep_bs128            74.478     21.527101 10.278589          15.668   44.55  701.953666\n",
      "       resnet101_imagenet-1k_300ep_bs256            73.476     21.563749  9.665024          15.668   44.55  706.896782\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "file_path = 'exp_summary.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select relevant columns\n",
    "columns = ['exp_name', 'eval_top1', 'power_consumption(kWh)', 'espm', 'training_flops', 'params', 'throughput']\n",
    "\n",
    "# Get top 5 min and max SAM_watt entries\n",
    "min_sam_table = df.nsmallest(5, 'espm')[columns].copy()\n",
    "max_sam_table = df.nlargest(5, 'espm')[columns].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "min_sam_table.columns = ['Experiment Name', 'Accuracy (Top-1)', 'Energy (kWh)', 'ESPM', 'training_flops', 'params', 'throughput']\n",
    "max_sam_table.columns = ['Experiment Name', 'Accuracy (Top-1)', 'Energy (kWh)', 'ESPM', 'training_flops', 'params', 'throughput']\n",
    "\n",
    "# Print the tables\n",
    "print(\"🔽 Top 5 Minimum ESPM Entries:\\n\")\n",
    "print(min_sam_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n🔼 Top 5 Maximum ESPM Entries:\\n\")\n",
    "print(max_sam_table.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "escade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
